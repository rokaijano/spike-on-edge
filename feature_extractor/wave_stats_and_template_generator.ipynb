{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "animated-growing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nnclr import *\n",
    "import tensorflow as tf \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import time \n",
    "import scipy\n",
    "import sklearn\n",
    "from utils import *\n",
    "\n",
    "from waveform_data import WaveFormDataset\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-flight",
   "metadata": {},
   "source": [
    "# Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ordered-great",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "DSET_PATH =  \"..\\\\data\"\n",
    "DSET_NAME = \"hybrid_static_siprobe_64C_600_S12\"\n",
    "GENERATE_TEMPLATES = True\n",
    "\n",
    "file_id = \"ckpt_1654866143\" # \n",
    "\n",
    "encoder = tf.keras.models.load_model(\"checkpoints\\\\\"+file_id+\"\\\\encoder_high_acc.h5\", custom_objects={\"ResidualBlock\":ResidualBlock, \"LatentLayer\":LatentLayer, \"RandomBrightness\":RandomBrightness})\n",
    "\n",
    "encoder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "unexpected-length",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_encoded_dataset(data_gen, enc, iterate=20, supervised=False):\n",
    "    \n",
    "    main_res = []\n",
    "    target_samples = []\n",
    "    supervised_arr = []\n",
    "    \n",
    "    it = iter(data_gen)\n",
    "    for i in range(iterate):\n",
    "        \n",
    "        print(f\"Iterating ... {i}\", end=\"\\r\")\n",
    "        try:\n",
    "            sample = next(it)\n",
    "        except Exception as e:\n",
    "            print(f\"Maximum data {e}\")\n",
    "            break #continue\n",
    "        \n",
    "        target_samples.extend(tf.squeeze(sample[1]))\n",
    "        \n",
    "        if supervised: # if supervised we get a tuple as X as (sample, ch)\n",
    "            \n",
    "            supervised_arr.extend(sample[0][1])\n",
    "            \n",
    "                #target_samples.extend(tf.squeeze(sample[1]))\n",
    "            sample = sample[0]\n",
    "            \n",
    "        result = enc(sample[0])\n",
    "\n",
    "        result = tf.reshape(result, [-1,width])\n",
    "\n",
    "\n",
    "        main_res.extend(result)\n",
    "\n",
    "    target_samples = np.array(target_samples)\n",
    "    contrastive_labels = tf.zeros(target_samples.shape[0]) # batch size \n",
    "\n",
    "    unique_batch_values = np.unique(target_samples, axis=0) #\n",
    "\n",
    "    for i in range(unique_batch_values.shape[0]):\n",
    "\n",
    "        eq = tf.equal(unique_batch_values[i], target_samples)\n",
    "\n",
    "        new_labels = tf.math.reduce_all(eq, axis=-1)\n",
    "        contrastive_labels = tf.where(new_labels, i, contrastive_labels)\n",
    "\n",
    "\n",
    "    main_labels = np.array(contrastive_labels)\n",
    "    main_res = np.array(main_res)\n",
    "    \n",
    "    if supervised:\n",
    "        return main_res, main_labels, np.array(supervised_arr)\n",
    "    \n",
    "    return main_res, main_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "strong-granny",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "def visualize(main_res, main_labels, one_hot=False, use_pca=False, max_data = 1000, perplexity=30):\n",
    "    import matplotlib.pyplot as plt \n",
    "    from sklearn.decomposition import PCA\n",
    "    from sklearn.manifold import TSNE\n",
    "    pca = PCA(n_components=2)\n",
    "    tsne = TSNE(n_components=2, learning_rate='auto', init='random',random_state=0, verbose=True, perplexity=perplexity)\n",
    "\n",
    "\n",
    "    def res(x, label):\n",
    "        \n",
    "        uni = np.unique(label)\n",
    "        \n",
    "        for u in uni:\n",
    "            \n",
    "            xu = x[label==u]\n",
    "            rgb = np.random.rand(3,)\n",
    "            plt.scatter(xu[:,0], xu[:,1], color=[rgb])\n",
    "\n",
    "            plt.show()\n",
    "\n",
    "    mres = main_res[:max_data]\n",
    "    \n",
    "    if use_pca:\n",
    "        result = pca.fit_transform(mres)\n",
    "    else:\n",
    "        \n",
    "        pca = PCA(n_components=10)\n",
    "        mres = pca.fit_transform(mres)\n",
    "        result = tsne.fit_transform(mres)\n",
    "    \n",
    "    mlabels = main_labels[:max_data]\n",
    "    \n",
    "    if one_hot:\n",
    "        mlabels = np.argmax(mlabels, axis=-1)\n",
    "            \n",
    "    res(result, mlabels)\n",
    "    \n",
    "#visualize(main_res, main_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "welcome-elevation",
   "metadata": {},
   "source": [
    "## Check similarity w/ and w/o channel distance correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "laughing-guatemala",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterating ... 19\r"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "dset3 = WaveFormDataset([os.path.join(DSET_PATH, DSET_NAME)], batch_size=64, supervised=True, min_snr=0, use_cache=False)\n",
    "dset_w_idx = dset3()\n",
    "main_res, main_labels, ch = get_encoded_dataset(dset_w_idx, encoder, iterate=20, supervised=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "crazy-curtis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58]\n",
      "(64000, 32)\n",
      "(64000,)\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-13ea5dedaf74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_res\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmain_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32massert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main_labels = main_labels.astype(np.int32)\n",
    "print(np.unique(main_labels))\n",
    "print(main_res.shape)\n",
    "print(main_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-circus",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "\n",
    "from sklearn.neighbors import DistanceMetric\n",
    "dist = DistanceMetric.get_metric('manhattan')  # euclidean or manhattan\n",
    "\n",
    "clusters = [] \n",
    "cluster_means = []\n",
    "cluster_positions = []\n",
    "\n",
    "\n",
    "\n",
    "for l in np.unique(main_labels):\n",
    "    \n",
    "    means = np.mean(main_res[main_labels == l], axis=0)\n",
    "    pos = np.mean(ch[main_labels == l])\n",
    "    cluster_means.append(means)\n",
    "    cluster_positions.append(pos)\n",
    "    \n",
    "cluster_means = np.array(cluster_means)\n",
    "cluster_positions = np.array(cluster_positions)\n",
    "\n",
    "\n",
    "## Similarity #################\n",
    "\n",
    "sim = dist.pairwise(cluster_means)\n",
    "sim = sim - np.min(sim, axis=0)\n",
    "sim = sim / np.max(sim, axis=0)\n",
    "\n",
    "sim = 1 - sim\n",
    "\n",
    "\n",
    "## Position ####################\n",
    "cluster_positions = np.expand_dims(cluster_positions, 1)\n",
    "pos = scipy.spatial.distance_matrix(cluster_positions, cluster_positions)\n",
    "pos = pos - np.min(pos)\n",
    "pos = pos / np.max(pos)\n",
    "pos = 1 - pos\n",
    "pos = pos #**3\n",
    "\n",
    "## Combination #################\n",
    "comb = sim * pos\n",
    "comb = comb / np.max(comb)\n",
    "\n",
    "\n",
    "plt.matshow(sim)\n",
    "plt.title(\"Similarity/Distance matrix - Sim\")\n",
    "\n",
    "plt.matshow(pos)\n",
    "plt.title(\"Channel 1-distance normalized - CH\")\n",
    "\n",
    "plt.matshow(comb)\n",
    "plt.title(\"Sim matrix combined with distance - Sim * CH\")\n",
    "\n",
    "max_val = np.max(comb - np.diag(np.diag(comb)))\n",
    "print(f\"Max value (excluding diagonals): {max_val}\")\n",
    "\n",
    "\n",
    "comb2 = np.where(comb < .8, 0, comb)\n",
    "plt.matshow(comb2)\n",
    "plt.title(\"Sim matrix combined with distance - .8 thresholded\")\n",
    "\n",
    "remaining_candidates = (np.count_nonzero(comb2) - comb2.shape[0]) / 2 \n",
    "\n",
    "print(f\"Remaining candidates: {remaining_candidates}, accuracy could be max: {(comb2.shape[0]-remaining_candidates) / comb2.shape[0]}\")\n",
    "\n",
    "## Getting the templates ################\n",
    "templates = dset3._waveforms[0].numpy()\n",
    "w_min = tf.math.reduce_min(templates, keepdims=True, axis=-1)\n",
    "w_max = tf.math.reduce_max(templates, keepdims=True, axis=-1)\n",
    "templates = (templates - w_min) / (w_max - w_min)\n",
    "\n",
    "\n",
    "## Template embedding similarity ##################\n",
    "\n",
    "t_sim = dist.pairwise(templates, templates) \n",
    "t_sim = t_sim - np.min(t_sim, axis=0)\n",
    "t_sim = t_sim / np.max(t_sim, axis=0)\n",
    "t_sim = 1 - t_sim\n",
    "\n",
    "plt.matshow(t_sim)\n",
    "plt.title(\"Template 1-distance (normalized) \")\n",
    "\n",
    "\n",
    "templates = encoder(templates)\n",
    "## this is not a good solution but for trying out it will do it \n",
    "print(templates.shape)\n",
    "#templates = templates[:cluster_means.shape[0]]\n",
    "templates = templates[templates.shape[0]-cluster_means.shape[0]:]\n",
    "print(templates.shape)\n",
    "templates = templates[::-1]\n",
    "\n",
    "## Template embedding similarity ##################\n",
    "\n",
    "t_sim = dist.pairwise(templates,templates) \n",
    "t_sim = t_sim - np.min(t_sim, axis=0)\n",
    "t_sim = t_sim / np.max(t_sim, axis=0)\n",
    "t_sim = 1 - t_sim\n",
    "\n",
    "plt.matshow(t_sim)\n",
    "plt.title(\"Template embedding 1-distance (normalized) \")\n",
    "\n",
    "## Mean Embedding - template similarity similarity ################\n",
    "\n",
    "diff_sim = t_sim * sim\n",
    "plt.matshow(diff_sim)\n",
    "plt.title(\" Template - Embedding similarity\")\n",
    "\n",
    "## Mean - template distance #############\n",
    "\n",
    "mt_sim = dist.pairwise(templates,cluster_means) \n",
    "mt_sim = mt_sim - np.min(mt_sim, axis=0)\n",
    "mt_sim = mt_sim / np.max(mt_sim, axis=0)\n",
    "mt_sim = 1 - mt_sim\n",
    "\n",
    "plt.matshow(mt_sim)\n",
    "plt.xlabel(\"Templates\")\n",
    "plt.ylabel(\"Cluster mean\")\n",
    "plt.title(\"Template and mean cluster 1-distance (normalized) - Mt\")\n",
    "\n",
    "\n",
    "corrected_sim = np.abs(sim - mt_sim)\n",
    "plt.matshow(corrected_sim)\n",
    "plt.title(\"Difference between Sim and Mt - | Sim - mt_sim |\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulated-domestic",
   "metadata": {},
   "source": [
    "# CALCULATE AUC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "african-filling",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "thresholds = [x/100 for x in range(0, 100, 5)]\n",
    "\n",
    "def get_score(corr, thresh):\n",
    "    \n",
    "    shape_0 = corr.shape[0]\n",
    "    corr2 = np.where(corr < thresh, 0, corr)\n",
    "\n",
    "    remaining_candidates = (np.count_nonzero(corr2) - shape_0) \n",
    "    \n",
    "    total_comb = shape_0**2\n",
    "\n",
    "    acc = max(0, (total_comb-remaining_candidates)) / total_comb\n",
    "    \n",
    "    return acc \n",
    "\n",
    "accs = [get_score(comb, x) for x in thresholds]\n",
    "\n",
    "print(f'computed AUC: {auc(thresholds,accs)}')\n",
    "\n",
    "plt.plot(thresholds, accs)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "welcome-estate",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[t-SNE] Computing 91 nearest neighbors...\n",
      "[t-SNE] Indexed 20000 samples in 0.052s...\n",
      "[t-SNE] Computed neighbors for 20000 samples in 1.619s...\n",
      "[t-SNE] Computed conditional probabilities for sample 1000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 2000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 3000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 4000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 5000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 6000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 7000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 8000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 9000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 10000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 11000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 12000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 13000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 14000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 15000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 16000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 17000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 18000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 19000 / 20000\n",
      "[t-SNE] Computed conditional probabilities for sample 20000 / 20000\n",
      "[t-SNE] Mean sigma: 0.075913\n",
      "[t-SNE] KL divergence after 250 iterations with early exaggeration: 85.051460\n",
      "[t-SNE] KL divergence after 1000 iterations: 1.905840\n"
     ]
    }
   ],
   "source": [
    "%matplotlib qt\n",
    "plt.title(\"YT\")\n",
    "visualize(main_res, main_labels, one_hot=False, max_data =20000, perplexity=30, use_pca=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-coach",
   "metadata": {},
   "source": [
    "# Generate embeddings for the templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "spiritual-relief",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "datasets_list = [\"hybrid_static_siprobe_64C_600_S11\", \"hybrid_static_siprobe_64C_600_S12\", \"REC_32C_600S_31\", \"1103_1_1\"]\n",
    "full_datasets = [os.path.join(\"..\\\\data\\\\\", x) for x in datasets_list]\n",
    "\n",
    "template_res = dset3._waveforms\n",
    "\n",
    "if GENERATE_TEMPLATES:\n",
    "    for i, tr in enumerate(template_res):\n",
    "\n",
    "        for j, we in enumerate(tr):\n",
    "            path = os.path.join(\"..\\\\supervised\\\\data\\\\\", datasets_list[i], \"waveform_embeddings\")\n",
    "            os.makedirs(path, exist_ok=True)\n",
    "            \n",
    "            np.save(os.path.join(path, str(j)+\".npy\"), we)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
